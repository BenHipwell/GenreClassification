{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I have stated all of the imports necessary for this project at the very beginning to keep them together.\n",
    "\n",
    "Included in this is the comet_ml package that I am using to track the performance of the models as I develop them further, helping to improve their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import getpass\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import comet_ml\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import random\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record the performance of each model, I am once again using comet_ml to track each training experiment and its outcome which is very useful when tweaking the models and their hyperparameters and observing the difference in performance. \n",
    "\n",
    "The code below creates a new experiment for me to record this session within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_api_key=os.environ.get(\"COMET_API_KEY\")\n",
    "comet_api_key=''\n",
    "\n",
    "experiment = comet_ml.Experiment(api_key=comet_api_key, workspace=\"benhipwell\", project_name=\"cw2\", auto_metric_logging=True, auto_output_logging=True)\n",
    "# experiment.set_name('benhipwellcomp6252cw2')\n",
    "\n",
    "# if comet_api_key is None:\n",
    "#   comet_api_key=getpass.getpass(\"Enter key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of training machine learning models using audio files is to generate fixed length segments of the time series to make up the dataset.\n",
    "\n",
    "Below I have created a custom PyTorch dataset that takes in the dataset of full length audio files, breaks them down into segments with their respective genre and stores them accordingly so that they can be used later on for training a model. \n",
    "\n",
    "In this case, I am extracting MFCC features from the wavelengths of each of the segments instead of using the raw audio as it provides an improved training accuracy and is much more time and space efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSegmentDataset(Dataset):\n",
    "    def __init__(self, root_dir = None, segment_length = 11000):\n",
    "        self.root_dir = root_dir\n",
    "        self.segment_length = segment_length\n",
    "        self.segments = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Loads and preprocesses the audio dataset\n",
    "        if root_dir:\n",
    "            self.classes = sorted(os.listdir(root_dir))\n",
    "\n",
    "            for idx, genre in enumerate(self.classes):\n",
    "                genre_folder = os.path.join(self.root_dir, genre)\n",
    "                files = os.listdir(genre_folder)\n",
    "                for file in files:\n",
    "                    filepath = os.path.join(genre_folder, file)\n",
    "                    waveform, sample_rate = torchaudio.load(filepath)\n",
    "                    num_samples = waveform.size(1)\n",
    "                    num_segments = num_samples // segment_length\n",
    "                    # breaks each track down into fixed sized segments\n",
    "                    for i in range(num_segments):\n",
    "                        start_idx = i * segment_length\n",
    "                        end_idx = start_idx + segment_length\n",
    "                        segment = waveform[:, start_idx:end_idx]\n",
    "                        # extracts MFCC features\n",
    "                        segment = torch.tensor(librosa.feature.mfcc(y=segment.squeeze(0).numpy(), sr=sample_rate)).to('cuda')\n",
    "                        self.segments.append(segment)\n",
    "                        self.labels.append(idx)\n",
    "            \n",
    "            # shuffles the dataset for later splitting into train, test and validation sets\n",
    "            combined = list(zip(self.segments, self.labels))\n",
    "            random.shuffle(combined)\n",
    "            self.segments[:], self.labels[:] = zip(*combined)\n",
    "\n",
    "    # returns the length of the segments list\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    # returns the respective segment and label given the index\n",
    "    def __getitem__(self, idx):\n",
    "        segment = self.segments[idx]\n",
    "        label = self.labels[idx]\n",
    "        return segment, torch.tensor(label, dtype=torch.long).to('cuda')\n",
    "\n",
    "    # adds new segments and labels to the dataset\n",
    "    def add_segment(self, new_segments, new_labels):\n",
    "        self.segments.extend(new_segments)\n",
    "        self.labels.extend(new_labels)\n",
    "\n",
    "    # to check and make sure that all of the segmments are of the same shape\n",
    "    def check_segment_shapes(self):\n",
    "        if len(set(segment.shape for segment in self.segments)) != 1:\n",
    "            raise ValueError(\"Segments have different shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialises the dataset, taking the root directory of the audio file dataset and the size of which fixed sized segments should be created. \n",
    "\n",
    "To confirm that the data has been loaded correctly, it displays the classes identified and the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data/genres_original/'\n",
    "segment_length = 110000\n",
    "\n",
    "dataset = AudioSegmentDataset(root_dir, segment_length)\n",
    "\n",
    "print(dataset.classes)\n",
    "\n",
    "total_size = len(dataset)\n",
    "print(f'Dataset size: {total_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using 'random_split()', I have had to manually split the dataset into other AudioSegment Datasets to allow for the use of the custom functions that a Dataset subset from 'random_split' could not provide. This is due to the later addition of augmented data that requires the use of the 'add_segment()' function. \n",
    "\n",
    "As the dataset has been shuffled on initialisation, this method of indexing to split up the dataset is viable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "print(f'Training set size: {train_size}')\n",
    "print(f'Validation set size: {val_size}')\n",
    "print(f'Testing set size: {test_size}')\n",
    "\n",
    "train_set = AudioSegmentDataset()\n",
    "val_set = AudioSegmentDataset()\n",
    "test_set = AudioSegmentDataset()\n",
    "\n",
    "train_set.segments = dataset.segments[:train_size]\n",
    "train_set.labels = dataset.labels[:train_size]\n",
    "\n",
    "val_set.segments = dataset.segments[train_size:train_size + val_size]\n",
    "val_set.labels = dataset.labels[train_size:train_size + val_size]\n",
    "\n",
    "test_set.segments = dataset.segments[train_size + val_size:]\n",
    "test_set.labels = dataset.labels[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using data loaders to create batches of the dataset for model training, it is important to ensure all of the data has been loaded correctly, where all of the segments are of the expected shape. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.check_segment_shapes()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my implementation of a LSTM RNN model, which takes:\n",
    "\n",
    "    input_size: number of expected features within the input\n",
    "    hidden_size: number of features in the hidden state\n",
    "    num_layers: number of recurrent layers in the LSTM\n",
    "    output_size: size of the output layer = number of output features after the last hidden layer\n",
    "    batch_size: size of each batch of data provided to the model\n",
    "\n",
    "This model makes use of LSTM layers to learn long-term dependencies in the input data, especially in time series tasks such as this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initialize h0 with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # initialize c0 with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # forward propagate LSTM, with an output of shape (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is function to carry out the training of the LSTM model, containing the training loop. This is a very standard implementation of the training loop, identical to that used in CW1 as it does not need to be adapted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_func, epochs, name):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        print(f\"epoch {epoch + 1}/{epochs}, loss: {running_loss / len(train_loader)}\")\n",
    "        experiment.log_metrics({f'loss_{name}': running_loss / len(train_loader)}, epoch=epoch)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This carries out the evaluation of a given model against a given testing or validation set. \n",
    "\n",
    "Similar to the training loop, the evaluation function is also identical to that used in the previous coursework as it is unaffected by the differing data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_func, name):\n",
    "    \n",
    "    print(f\"evaluating: {name}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    print(f\"accuracy: {accuracy}, average loss: {avg_loss}\")\n",
    "    experiment.log_metrics({f\"test_accuracy_{name}\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This firstly sets the parameters to train the first model that simply uses the provided dataset of audio files that have been processed into MFCC features of fixed sized segments that make up the audio tracks. Where possible, it is very helpful to group parameters together to be able to adjust them easily during testing and tweaking of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0007\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "input_size = 215\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "num_classes = 10\n",
    "epochs = 60\n",
    "\n",
    "rnn = LSTMModel(input_size, hidden_size, num_layers, num_classes, batch_size=batch_size).cuda()\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "train(rnn, train_loader, optimizer, loss_func, epochs=epochs, name='rnn_train')\n",
    "evaluate(rnn, test_loader, loss_func, name=f'rnn_without_GAN_{epochs}_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto the second part of this project, the implementation of a GAN will follow. For this project, instead of the methods of either training separate GANs for each class in the dataset or assigning random class labels to generated data, I have made use of Conditional GANs. \n",
    "\n",
    "To begin with, the implementation of the Generator part of the GAN which is responsible for generating augmented data from noise and a provided label. This label decides how the Generator should produce new augmented data from the given noise. \n",
    "\n",
    "The architecture of this model makes use of Linear layers of varying size and LeakyReLU layers which have proved to be more effective than regular ReLU layers in this instance. The activation of tanh is optimal for use in the generator to ensure the generated outputs are of the format and scale of the real data, whilst providing symmetry around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size, label_size, embed_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_embeddings=label_size, embedding_dim=embed_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_size + embed_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, output_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        labels = labels.long()\n",
    "        # generates dense embedding for the labels so that they can be used within the model\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        # combines input noise and label embeddings to provide contextual label information\n",
    "        gen_input = torch.cat((noise, label_embedding), dim=1)\n",
    "        output = self.fc(gen_input)\n",
    "        # reshape output to match MFCC segment shape for training\n",
    "        return output.view(-1, 20, 215)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto the Discriminator, this is used to evaluate whether the generated output from the Generator is real or fake, eseentially becoming the 'critic'. This is essential for the training process of a GAN, where both of these models train together to provide the best possible augmented data. \n",
    "\n",
    "This is a near identical model architecture to the Generator, to keep them consistent whilst they train. The only change is the use of the Sigmoid activation function which is instead used for binary classification of whether the Discriminator believes the generated augmented data is real of fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, label_size, embed_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_embeddings=label_size, embedding_dim=embed_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size + embed_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        labels = labels.long()\n",
    "        # generates dense embedding for the labels so that they can be used within the model\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        input = input.view(input.size(0), -1)\n",
    "        # combines input data and label embeddings to combine raw data features with contextual label information\n",
    "        disc_input = torch.cat((input, label_embedding), dim=1)\n",
    "        output = self.fc(disc_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function contains the main training loop of the CGAN to generate augmented data from the original dataset. The key aspect is the simulataneous training of the above defined Generator and Discriminator learn from each other to have the ability to generate these new samples effectively. \n",
    "\n",
    "To help stop the Discriminator from dominating the Generator, I have added some noise to the Discriminator inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, device, dataloader, optimizer_G, optimizer_D, criterion, num_epochs, label_size, noise_size):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (audio, labels) in enumerate(dataloader):\n",
    "            real_data = audio.to(device) + 0.05 * torch.randn_like(audio)\n",
    "            real_labels = labels.to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_output = discriminator(real_data, real_labels)\n",
    "            real_loss = criterion(real_output, torch.ones(batch_size, 1, device=device))\n",
    "\n",
    "            fake_labels = torch.randint(0, label_size, (batch_size,), device=device)\n",
    "            noise = torch.randn(batch_size, noise_size, device=device)\n",
    "            fake_data = generator(noise, fake_labels) + 0.05 * torch.randn_like(audio)\n",
    "            fake_output = discriminator(fake_data.detach(), fake_labels)\n",
    "            fake_loss = criterion(fake_output, torch.zeros(batch_size, 1, device=device))\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            output = discriminator(fake_data, fake_labels)\n",
    "            g_loss = criterion(output, torch.ones(batch_size, 1, device=device))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss D: {d_loss.item()}, Loss G: {g_loss.item()}\")\n",
    "        experiment.log_metrics({f'loss_Generator': g_loss.item()}, epoch=epoch)\n",
    "        experiment.log_metrics({f'loss_Discriminator': d_loss.item()}, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets the parameters required for training the GAN, before initialising all of the required components and making use of the training loop. Note the Generator learning rate is higher to stop it from being dominated by the Discriminator and avoiding Mode Collapse.\n",
    "\n",
    "BCE Loss is used as to measure the distance between predicted probabilities of the Discriminator output and the actual binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 200\n",
    "label_size = 10\n",
    "embed_size = 50\n",
    "output_size = 4300\n",
    "g_lr = 0.001\n",
    "d_lr = 0.0006\n",
    "\n",
    "generator = Generator(noise_size, label_size, embed_size, output_size)\n",
    "discriminator = Discriminator(output_size, label_size, embed_size)\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=g_lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "criterion = nn.BCELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_gan(generator, discriminator, device, train_loader, optimizer_G, optimizer_D, criterion, num_epochs=50, label_size=label_size, noise_size=noise_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to generate new augmented data from the trained CGAN for each of the genre classes, with an equal amount of samples generated for each one, duplicating the original training set size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_samples(generator, num_samples, noise_size, num_classes, device='cuda'):\n",
    "    \n",
    "    generator.eval()\n",
    "    num_samples_per_class = num_samples // num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id in range(num_classes):\n",
    "            print(f'Augmenting for class: {id}')\n",
    "            # generate intput noise vector\n",
    "            noise = torch.randn(num_samples_per_class, noise_size, device=device)\n",
    "            # generate augmented data based on the label\n",
    "            labels = torch.Tensor(list(repeat(id, num_samples_per_class))).to('cuda')\n",
    "            synthetic_data = generator(noise, labels).to('cuda')\n",
    "            # make use of the custom function to add the synthetic data to only the train set\n",
    "            train_set.add_segment(synthetic_data, labels)\n",
    "\n",
    "# generate the same number of samples already in the train set, duplicating its size\n",
    "num_samples_to_generate = train_size\n",
    "num_classes = 10\n",
    "generate_audio_samples(generator, num_samples_to_generate, noise_size, num_classes, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After populating the train set with the synthetic data, the dataset it once again checked to ensure all of the data is of the same shape and therefore suitable to be used for model training. \n",
    "\n",
    "This makes use of the same parameters and conditions as the previously trained model, however with the train set being the only changing factor which now contains extra synthetic data. This consistency provides a better environment to measure the impact of the synthetic data on the training of a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.check_segment_shapes()\n",
    "\n",
    "train_loader_GAN = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "rnn_gan = LSTMModel(input_size, hidden_size, num_layers, num_classes, batch_size=batch_size).cuda()\n",
    "\n",
    "optimizer_gan = optim.Adam(rnn_gan.parameters(), lr=lr)\n",
    "train(rnn_gan, train_loader_GAN, optimizer_gan, loss_func, epochs=epochs, name='rnn_gan_train')\n",
    "evaluate(rnn_gan, test_loader, loss_func, name=f'rnn_with_GAN_{epochs}_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this makes use of comet_ml to track the hyperparameters used when training the two models so that they can be compared when running with changing parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\":batch_size,\n",
    "    \"learning_rate\":lr,\n",
    "    \"generator_learning_rate\" : g_lr,\n",
    "    \"discriminator_learning_rate\" : d_lr,\n",
    "    \"noise_size\": noise_size,\n",
    "    \"hidden_size\" : hidden_size,\n",
    "    \"num_layers\" : num_layers,\n",
    "    \"segment_length\" : segment_length,\n",
    "    \"epochs\" : epochs\n",
    "}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
